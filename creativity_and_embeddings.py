# -*- coding: utf-8 -*-
"""Creativity and Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ITHaNzvQi6xAgRGfmaOdAS1ky5n-VUwv

# 1. Introduction and Overview

This notebook provides a comprehensive framework for analyzing text data to study creativity in solutions using embedding models. It computes embeddings using multiple state-of-the-art models, visualize these embeddings, and apply various analytical techniques to identify and measure creative aspects of textual solutions.
The primary goals of this framework are to:

- Transform textual solutions into numerical vector representations (embeddings)
- Visualize similarities and differences between solutions
- Identify outliers that may represent highly creative or unique approaches
- Cluster similar solutions to understand common patterns
- Quantify the diversity and uniqueness of solution sets

By comparing the semantic positioning of solutions in embedding space, we can gain insights into creative thinking processes, solution diversity, and innovation patterns.
"""

# Text Embeddings Analysis for Creativity Research
# This notebook computes and analyzes embeddings from multiple models to study creativity in solutions

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import openai
from tqdm.notebook import tqdm
import plotly.express as px
import os
from scipy.spatial.distance import pdist, squareform
import matplotlib.patches as mpatches
from typing import List, Dict

# Display settings
pd.set_option('display.max_colwidth', None)

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "your_key_here"  # Replace with your API key. Costs should be $0.13 per 1M input tokens (that's a very low cost).

import pandas as pd
import re
import os
from google.colab import files
from tqdm import tqdm

tqdm.pandas()

"""First load your personal data. I suggest using a method similar to gdown by just providing your shareable ID from drive (viewer's rights to anyone with link)."""

!gdown 10QSmJ8qcFqg5iDXk90wf3QFtsBYIuHbF -O prosperity.csv #paste your share link id, make sure to also choose a name with extension that is the right one for you e.g., .csv. .xlsx

"""If you need to do a bit of preprocessing to your data, do it below. Ideally, you do it in a separate notebook and just load the perfect data for embedding. But as an example, here is a dataset of startup solutions that I prepare for the code.

Importantly, the code will assume that the text to embed is included in a single column. Ideally you will name that column ```text_to_embed``` but the code will allow you to specify the column name to embed.
"""

df = pd.read_csv("prosperity.csv")

"""# Make sure to execute all code below. No changes needed so far.

# Embedding functions

### OpenAI Embeddings

The get_openai_embeddings function connects to OpenAI's API to generate state-of-the-art text embeddings. OpenAI's latest embedding models (text-embedding-3-large) provide high-quality semantic representations that capture nuanced meanings and relationships between concepts, making them excellent for creativity research.
### EuroBERT Embeddings
The get_eurobert_embeddings function uses the EuroBERT model to generate embeddings that are particularly well-suited to European language contexts. EuroBERT offers robust representations that may capture certain cultural and contextual nuances in European languages that might be relevant for creativity research.
### SentenceBERT Embeddings
The get_sentencebert_embeddings function uses the SentenceBERT framework, which is specifically optimized for generating meaningful sentence-level embeddings. SentenceBERT models are fine-tuned specifically for semantic similarity tasks, making them particularly valuable for comparing creative solutions.
"""

def get_openai_embeddings(texts: List[str], model: str = "text-embedding-3-large") -> np.ndarray:
    """Compute text embeddings using OpenAI's latest embedding model."""
    openai.api_key = os.getenv("OPENAI_API_KEY")
    if not openai.api_key:
        raise ValueError("OpenAI API key not found. Please set it in the notebook.")

    embeddings = []
    batch_size = 100  # OpenAI recommends batching requests

    for i in tqdm(range(0, len(texts), batch_size), desc="Computing OpenAI embeddings"):
        batch = texts[i:i+batch_size]
        response = openai.embeddings.create(
            model=model,
            input=batch
        )
        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)

    return np.array(embeddings)

def get_eurobert_embeddings(texts: List[str], model_name: str = "EuroBERT/EuroBERT-210m") -> np.ndarray:
    """
    Compute text embeddings using EuroBERT with proper truncation handling.

    This function handles long texts by properly truncating them to the model's
    maximum sequence length while ensuring embeddings are correctly extracted.

    Args:
        texts: List of text strings to embed
        model_name: Name of the EuroBERT model to use

    Returns:
        NumPy array of embeddings
    """
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

    # Set device (GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    embeddings = []
    batch_size = 8  # Further reduced batch size to handle longer sequences (8K tokens)

    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Computing EuroBERT embeddings"):
            batch = texts[i:i+batch_size]

            # Tokenize with explicit truncation to handle long texts properly
            # Truncate to 8192 tokens since EuroBERT can handle up to 8K tokens
            encoded_input = tokenizer(
                batch,
                padding='max_length',  # Use consistent padding
                truncation=True,       # Enable truncation
                max_length=8192,       # Set maximum length to 8K tokens
                return_tensors='pt',   # Return PyTorch tensors
                return_attention_mask=True  # Get attention mask to handle padding properly
            )

            # Move tensors to device
            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}

            # Forward pass through the model
            outputs = model(**encoded_input)

            # Get the attention mask to properly handle padding
            attention_mask = encoded_input['attention_mask']

            # Different ways to get sentence embeddings:

            # Method 1: CLS token embedding (first token)
            # This is standard for BERT sentence representations
            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

            # Method 2 (alternative): Mean of all token embeddings, weighted by attention mask
            # This can sometimes provide better representations for long texts
            # mean_embeddings = []
            # for j in range(len(batch)):
            #     # Get the token embeddings for this sequence
            #     token_embeddings = outputs.last_hidden_state[j]
            #     # Get the attention mask for this sequence
            #     mask = attention_mask[j]
            #     # Compute the mean of the token embeddings, weighted by the mask
            #     masked_embeddings = token_embeddings * mask.unsqueeze(-1)
            #     sum_embeddings = torch.sum(masked_embeddings, dim=0)
            #     seq_length = torch.sum(mask)
            #     mean_embedding = sum_embeddings / seq_length
            #     mean_embeddings.append(mean_embedding.cpu().numpy())
            # embeddings.extend(mean_embeddings)

            # Using CLS token embeddings (Method 1)
            embeddings.extend(cls_embeddings)

    # Convert list to numpy array
    embeddings_array = np.array(embeddings)

    # Normalize embeddings (important for cosine similarity)
    norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
    normalized_embeddings = embeddings_array / norms

    return normalized_embeddings

def get_sentencebert_embeddings(texts: List[str], model_name: str = "all-mpnet-base-v2") -> np.ndarray:
    """Compute text embeddings using SentenceBERT."""
    # Load model
    model = SentenceTransformer(model_name)

    # Set device (GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Compute embeddings with normalization
    embeddings = model.encode(
        texts,
        batch_size=16,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True  # Normalize for cosine similarity
    )

    return embeddings

### Note from Leo: It seems the Longformer embeddings don't really work and I don't why.
def get_longformer_embeddings(texts: List[str], model_name: str = "allenai/longformer-base-4096") -> np.ndarray:
    """Compute text embeddings using Longformer for longer documents."""
    # Import required libraries
    from transformers import AutoTokenizer, AutoModel

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # Set device (GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    embeddings = []
    batch_size = 8  # Smaller batch size due to longer sequences

    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Computing Longformer embeddings"):
            batch = texts[i:i+batch_size]

            # Tokenize with appropriate settings for Longformer
            encoded_input = tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=4096,  # Longformer can handle 4096 tokens
                return_tensors='pt'
            )

            # Move to device
            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}

            # Forward pass
            outputs = model(**encoded_input)

            # Use the [CLS] token embedding as the document embedding
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.extend(batch_embeddings)

    # Convert to array and normalize
    embeddings_array = np.array(embeddings)
    norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
    normalized_embeddings = embeddings_array / norms

    return normalized_embeddings

"""#########################
# Visualization Functions #
#########################
"""

def reduce_with_pca(embeddings: np.ndarray, n_components: int = 2) -> np.ndarray:
    """Reduce dimensionality using PCA."""
    pca = PCA(n_components=n_components)
    return pca.fit_transform(embeddings)

def reduce_with_tsne(embeddings: np.ndarray, n_components: int = 2, perplexity: int = 30) -> np.ndarray:
    """Reduce dimensionality using t-SNE."""
    # Adjust perplexity if it's larger than the number of samples
    n_samples = embeddings.shape[0]
    if perplexity >= n_samples:
        # Set perplexity to n_samples/3 or 5, whichever is smaller but at least 2
        adjusted_perplexity = max(2, min(5, n_samples // 3))
        print(f"Warning: Perplexity ({perplexity}) must be less than n_samples ({n_samples}). "
              f"Adjusting perplexity to {adjusted_perplexity}.")
        perplexity = adjusted_perplexity

    # Use max_iter instead of n_iter to avoid deprecation warning
    tsne = TSNE(n_components=n_components, perplexity=perplexity, max_iter=1000, random_state=42)
    return tsne.fit_transform(embeddings)

def visualize_embeddings(embeddings: np.ndarray, labels=None, title: str = "Embedding Visualization",
                         reduction_methods: List[str] = ["PCA", "t-SNE"], color_by=None):
    """Visualize embeddings using dimensionality reduction."""
    fig, axes = plt.subplots(1, len(reduction_methods), figsize=(18, 7))

    if len(reduction_methods) == 1:
        axes = [axes]

    for i, method in enumerate(reduction_methods):
        if method == "PCA":
            reduced_embeddings = reduce_with_pca(embeddings)
            method_title = "PCA"
        elif method == "t-SNE":
            reduced_embeddings = reduce_with_tsne(embeddings)
            method_title = "t-SNE"
        else:
            raise ValueError(f"Unknown reduction method: {method}")

        # Create scatter plot
        df_plot = pd.DataFrame(reduced_embeddings, columns=["x", "y"])

        if labels is not None:
            df_plot["label"] = labels

        if color_by is not None:
            scatter = axes[i].scatter(df_plot["x"], df_plot["y"], c=color_by, cmap="coolwarm", alpha=0.8, s=100)
            plt.colorbar(scatter, ax=axes[i], label="Value")
        else:
            axes[i].scatter(df_plot["x"], df_plot["y"], alpha=0.8, s=100)

        axes[i].set_title(f"{title} - {method_title}", fontsize=14)
        axes[i].set_xlabel("Dimension 1", fontsize=12)
        axes[i].set_ylabel("Dimension 2", fontsize=12)
        axes[i].grid(True, linestyle="--", alpha=0.7)

    plt.tight_layout()
    plt.show()

def create_interactive_plot(embeddings: np.ndarray, df: pd.DataFrame, method: str = "t-SNE",
                           color_by: str = None, hover_data: List[str] = None):
    """Create an interactive visualization of embeddings."""
    if method == "PCA":
        reduced_embeddings = reduce_with_pca(embeddings)
    elif method == "t-SNE":
        reduced_embeddings = reduce_with_tsne(embeddings)
    else:
        raise ValueError(f"Unknown reduction method: {method}")

    df_plot = pd.DataFrame(reduced_embeddings, columns=["dim_1", "dim_2"])

    if hover_data:
        for col in hover_data:
            if col in df.columns:
                df_plot[col] = df[col].values

    # Add color column if specified
    if color_by and color_by in df.columns:
        df_plot[color_by] = df[color_by].values

    fig = px.scatter(
        df_plot,
        x="dim_1",
        y="dim_2",
        color=color_by if color_by else None,
        hover_data=hover_data,
        title=f"Embedding Visualization using {method}",
        labels={"dim_1": "Dimension 1", "dim_2": "Dimension 2"},
        template="plotly_white"
    )

    fig.update_traces(marker=dict(size=10, opacity=0.8))
    fig.update_layout(height=700, width=1000)

    fig.show()

def compute_cosine_similarity_matrix(embeddings: np.ndarray) -> np.ndarray:
    """Compute cosine similarity matrix between all embeddings."""
    similarity_matrix = cosine_similarity(embeddings)
    return similarity_matrix

def visualize_similarity_matrix(similarity_matrix: np.ndarray, labels=None, title: str = "Cosine Similarity Matrix"):
    """Visualize cosine similarity matrix as a heatmap."""
    plt.figure(figsize=(12, 10))

    # Create heatmap
    sns.heatmap(
        similarity_matrix,
        annot=False,
        cmap="coolwarm",
        xticklabels=labels if labels else False,
        yticklabels=labels if labels else False
    )

    plt.title(title, fontsize=15)
    plt.tight_layout()
    plt.show()

"""#############################
# Creativity Analysis Tools #
#############################
"""

def identify_creative_outliers(embeddings: np.ndarray, threshold: float = 1.5) -> List[int]:
    """Identify potential creative outliers based on distance to centroid."""
    # Compute centroid
    centroid = np.mean(embeddings, axis=0)

    # Compute distances to centroid
    distances = np.sqrt(np.sum((embeddings - centroid)**2, axis=1))

    # Identify outliers
    mean_dist = np.mean(distances)
    std_dist = np.std(distances)
    outlier_indices = np.where(distances > mean_dist + threshold * std_dist)[0]

    return outlier_indices.tolist()

def cluster_solutions(embeddings: np.ndarray, n_clusters: int = 5) -> np.ndarray:
    """Cluster solutions to identify groups of similar approaches."""
    # Perform K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings)

    return cluster_labels

def visualize_clusters(embeddings: np.ndarray, cluster_labels: np.ndarray,
                      method: str = "PCA", title: str = "Solution Clusters"):
    """Visualize clusters of solutions."""
    if method == "PCA":
        reduced_embeddings = reduce_with_pca(embeddings)
    elif method == "t-SNE":
        reduced_embeddings = reduce_with_tsne(embeddings)
    else:
        raise ValueError(f"Unknown reduction method: {method}")

    plt.figure(figsize=(12, 10))

    # Create scatter plot with clusters colored
    scatter = plt.scatter(
        reduced_embeddings[:, 0],
        reduced_embeddings[:, 1],
        c=cluster_labels,
        cmap="tab10",
        alpha=0.8,
        s=120
    )

    # Add legend
    n_clusters = len(np.unique(cluster_labels))
    legend_handles = [mpatches.Patch(color=scatter.cmap(scatter.norm(i)), label=f"Cluster {i}")
                     for i in range(n_clusters)]
    plt.legend(handles=legend_handles, fontsize=12)

    plt.title(title, fontsize=15)
    plt.xlabel("Dimension 1", fontsize=12)
    plt.ylabel("Dimension 2", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.tight_layout()
    plt.show()

def compute_diversity_metrics(embeddings: np.ndarray) -> Dict[str, float]:
    """Compute metrics for measuring the diversity of solutions."""
    # Compute centroid
    centroid = np.mean(embeddings, axis=0)

    # Compute distances to centroid
    distances = np.sqrt(np.sum((embeddings - centroid)**2, axis=1))

    # Compute pairwise distances
    pairwise_distances = squareform(pdist(embeddings, metric='cosine'))

    # Compute diversity metrics
    metrics = {
        "mean_distance_to_centroid": np.mean(distances),
        "max_distance_to_centroid": np.max(distances),
        "mean_pairwise_distance": np.mean(pairwise_distances),
        "max_pairwise_distance": np.max(pairwise_distances),
        "variance_explained_ratio": np.var(distances) / np.sum(np.var(embeddings, axis=0))
    }

    return metrics

def compare_similarity_matrices(embedding_results: Dict, labels=None, title: str = "Cosine Similarity Comparison",
                           colormaps: List[str] = ["coolwarm", "coolwarm", "coolwarm"]):
    """Compare cosine similarity matrices from different embedding models side by side.

    Args:
        embedding_results: Dictionary containing embedding results from multiple models
        labels: Optional labels for the axes
        title: Overall title for the comparison
        colormaps: List of colormaps to use for each model (should match the number of models)
    """
    # Get the models we have results for
    models = list(embedding_results.keys())
    n_models = len(models)

    # Create a figure with n_models subplots in one row
    fig, axes = plt.subplots(1, n_models, figsize=(n_models * 5, 4))

    # If there's only one model, make axes iterable
    if n_models == 1:
        axes = [axes]

    # Ensure we have enough colormaps
    if len(colormaps) < n_models:
        colormaps = colormaps + [colormaps[-1]] * (n_models - len(colormaps))

    # Find global min and max for consistent color scaling
    all_sim_values = []
    for model in models:
        all_sim_values.extend(embedding_results[model]["similarity_matrix"].flatten())
    vmin = min(all_sim_values)
    vmax = max(all_sim_values)

    # Plot each similarity matrix
    for i, model in enumerate(models):
        similarity_matrix = embedding_results[model]["similarity_matrix"]

        # Create heatmap on the current axis
        sns.heatmap(
            similarity_matrix,
            annot=False,
            cmap=colormaps[i],
            xticklabels=labels if labels else False,
            yticklabels=labels if labels else False,
            vmin=vmin,
            vmax=vmax,
            ax=axes[i]
        )

        axes[i].set_title(f"{model.capitalize()} Similarity", fontsize=12)

    # Add an overall title
    fig.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle
    plt.show()

def compute_individual_distances(
    embeddings: np.ndarray
) -> Dict[str, np.ndarray]:
    """
    For each solution embedding, compute:
      - dist_to_centroid: Euclidean distance to the mean embedding (centroid)
      - avg_pairwise_distance: average cosine distance to all other embeddings
    Returns a dict with two arrays of shape (n_samples,).
    """
    # 1. Centroid and distances to it
    centroid = np.mean(embeddings, axis=0)
    dist_to_centroid = np.linalg.norm(embeddings - centroid, axis=1)

    # 2. Pairwise cosine‐distance matrix
    pairwise_cosine = squareform(pdist(embeddings, metric='cosine'))

    # 3. Compute per‐row average excluding self (diagonal zeros)
    n = embeddings.shape[0]
    # sum of distances in each row divided by (n‑1)
    avg_pairwise_distance = pairwise_cosine.sum(axis=1) / (n - 1)

    return {
        "dist_to_centroid": dist_to_centroid,
        "avg_pairwise_distance": avg_pairwise_distance
    }

import numpy as np
import matplotlib.pyplot as plt

def plot_distance_heatmaps(embedding_results: Dict[str, dict],
                           figsize = (12, 2)):
    """
    For each embedding model in embedding_results, compute per‐solution distances
    (to centroid and to peers) and plot a 2×N heatmap.

    Args:
        embedding_results: dict mapping model name to dict with key "embeddings".
        figsize: size of each heatmap figure.
    """
    for model_name, result in embedding_results.items():
        embeddings = result["embeddings"]
        # compute distances
        d = compute_individual_distances(embeddings)
        dist_to_centroid      = d["dist_to_centroid"]
        avg_pairwise_distance = d["avg_pairwise_distance"]

        # stack into 2×N matrix
        dist_matrix = np.vstack([dist_to_centroid, avg_pairwise_distance])

        # plot
        plt.figure(figsize=figsize)
        plt.imshow(dist_matrix, aspect='auto')
        plt.yticks([0, 1], ["Dist to Centroid", "Avg Dist to Others"])
        plt.xticks(np.arange(dist_matrix.shape[1]), np.arange(dist_matrix.shape[1]))
        plt.xlabel("Solution Index")
        plt.title(f"{model_name.capitalize()} Distances Heatmap")
        plt.colorbar(label="Distance")
        plt.tight_layout()
        plt.show()

"""#######################
# Main Analysis Flow #
#######################
"""

def analyze_embeddings(df: pd.DataFrame, text_column: str = "text_to_embed"):
    """Analyze text embeddings for a given DataFrame."""
    # Check if the text column exists
    if text_column not in df.columns:
        raise ValueError(f"Column '{text_column}' not found in the DataFrame.")

    # Extract texts to embed
    texts = df[text_column].tolist()

    # Compute embeddings
    print("Computing embeddings using multiple models...")

    results = {}

    # OpenAI embeddings
    print("\nComputing OpenAI embeddings:")
    openai_embeddings = get_openai_embeddings(texts)
    results["openai"] = {
        "embeddings": openai_embeddings,
        "similarity_matrix": compute_cosine_similarity_matrix(openai_embeddings)
    }

    # EuroBERT embeddings
    print("\nComputing EuroBERT embeddings:")
    eurobert_embeddings = get_eurobert_embeddings(texts)
    results["eurobert"] = {
        "embeddings": eurobert_embeddings,
        "similarity_matrix": compute_cosine_similarity_matrix(eurobert_embeddings)
    }

    # SentenceBERT embeddings
    print("\nComputing SentenceBERT embeddings:")
    sentencebert_embeddings = get_sentencebert_embeddings(texts)
    results["sentencebert"] = {
        "embeddings": sentencebert_embeddings,
        "similarity_matrix": compute_cosine_similarity_matrix(sentencebert_embeddings)
    }

    # Longformer embeddings
    print("\nComputing Longformer embeddings:")
    longformer_embeddings = get_longformer_embeddings(texts)
    results["longformer"] = {
        "embeddings": longformer_embeddings,
        "similarity_matrix": compute_cosine_similarity_matrix(longformer_embeddings)
    }

    # Visualizations
    print("\nCreating visualizations...")

    # PCA and t-SNE visualizations for each model
    for model_name, model_results in results.items():
        print(f"\nVisualizing {model_name} embeddings:")
        visualize_embeddings(
            model_results["embeddings"],
            title=f"{model_name.capitalize()} Embeddings",
            reduction_methods=["PCA", "t-SNE"]
        )

        # Visualize similarity matrix
        print(f"\nVisualizing {model_name} similarity matrix:")
        visualize_similarity_matrix(
            model_results["similarity_matrix"],
            title=f"{model_name.capitalize()} Cosine Similarity Matrix"
        )

    return results

def analyze_creativity(df: pd.DataFrame, embeddings: np.ndarray, text_column: str = "text_to_embed"):
    """Perform creativity analysis on embeddings."""
    print("Performing creativity analysis...")

    # 1. Identify potential creative outliers
    outlier_indices = identify_creative_outliers(embeddings)
    print(f"\nPotential creative outliers (unusual solutions):")
    if outlier_indices:
        for idx in outlier_indices:
            print(f"Index {idx}: {df.iloc[idx][text_column][:100]}...")  # Print first 100 chars
    else:
        print("No significant outliers found.")

    # 2. Cluster solutions
    # More conservative cluster selection for small datasets
    n_clusters = min(3, max(2, len(df) // 3))  # Ensure appropriate number of clusters for sample size
    print(f"Using {n_clusters} clusters for analysis based on dataset size ({len(df)} samples)")
    cluster_labels = cluster_solutions(embeddings, n_clusters=n_clusters)

    # Add cluster labels to DataFrame
    df_analysis = df.copy()
    df_analysis["cluster"] = cluster_labels

    print(f"\nSolution clusters:")
    for cluster_id in range(n_clusters):
        cluster_size = np.sum(cluster_labels == cluster_id)
        print(f"Cluster {cluster_id}: {cluster_size} solutions")

    # 3. Visualize clusters
    visualize_clusters(embeddings, cluster_labels, method="PCA", title="Solution Clusters (PCA)")
    visualize_clusters(embeddings, cluster_labels, method="t-SNE", title="Solution Clusters (t-SNE)")

    # 4. Compute diversity metrics
    diversity_metrics = compute_diversity_metrics(embeddings)

    print("\nDiversity metrics:")
    for metric, value in diversity_metrics.items():
        print(f"{metric}: {value:.4f}")

    # 5. Interactive visualization with cluster coloring
    create_interactive_plot(
        embeddings,
        df_analysis,
        method="t-SNE",
        color_by="cluster",
        hover_data=[text_column] + [col for col in df.columns if col != text_column]
    )

    return df_analysis

"""# Example Usage      


Sample code to demonstrate usage.

Create or load your DataFrame with a 'text_to_embed' column
Note: This is a small sample dataset for demonstration purposes.
For real-world creativity analysis, use a larger dataset (ideally 20+ samples)
```
data = {
    "text_to_embed": [
        "A mobile app that helps users track their carbon footprint by scanning product barcodes.",
        "A social network for connecting local farmers with consumers to reduce food miles.",
        "An AI system that optimizes building energy usage based on occupancy patterns.",
        "A platform that incentivizes public transportation usage through gamification.",
        "A blockchain-based system for transparent supply chain tracking of sustainable products.",
        "A wearable device that monitors air quality and suggests healthier routes for commuting.",
        "A community-based tool for sharing rarely used household items to reduce consumption.",
        "An educational platform teaching sustainable practices through interactive simulations.",
        "A system that converts food waste from restaurants into compost for urban gardens.",
        "A marketplace connecting businesses with surplus materials to artists who can repurpose them."
    ]
}

df = pd.DataFrame(data)

# Display the DataFrame
print("Sample data:")
display(df)

# Analyze embeddings
embedding_results = analyze_embeddings(df, text_column="text_to_embed")
```

Now doing it with my dataset:
"""

embedding_results = analyze_embeddings(df, text_column="text_to_embed") #specify the column name to embed

# Choose one embedding model for creativity analysis
df_with_clusters = analyze_creativity(
    df,
    embedding_results["openai"]["embeddings"],
    text_column="text_to_embed"
)

compare_similarity_matrices(
        embedding_results,
        title="Cosine Similarity Comparison Across Models",
    )

plot_distance_heatmaps(embedding_results)

def print_embedding_info(embedding_results: Dict):
    """Print a formatted summary of embedding information for all models."""
    models = list(embedding_results.keys())

    # Print header
    print("\n" + "="*80)
    print(f"{'EMBEDDING MODEL SUMMARY':^80}")
    print("="*80)
    print(f"{'Model':<15}{'Shape':<20}{'Dimensions':<15}{'Memory Usage':<20}{'Sample Count'}")
    print("-"*80)

    # Print information for each model
    for model in models:
        embeddings = embedding_results[model]["embeddings"]
        shape = embeddings.shape
        dimensions = shape[1]
        sample_count = shape[0]
        memory_mb = embeddings.nbytes / (1024 * 1024)

        print(f"{model.capitalize():<15}{str(shape):<20}{dimensions:<15}{f'{memory_mb:.2f} MB':<20}{sample_count}")

    print("-"*80)

    # Print comparison summary
    dimension_values = [embedding_results[model]["embeddings"].shape[1] for model in models]
    max_dim_model = models[dimension_values.index(max(dimension_values))]
    min_dim_model = models[dimension_values.index(min(dimension_values))]

    print(f"Highest dimensionality: {max_dim_model.capitalize()} ({max(dimension_values)} dimensions)")
    print(f"Lowest dimensionality: {min_dim_model.capitalize()} ({min(dimension_values)} dimensions)")
    print(f"Total samples: {embedding_results[models[0]]['embeddings'].shape[0]}")
    print("="*80)

print_embedding_info(embedding_results)
